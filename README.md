100Gaj AI Chatbot API Integration Guide
1. Project Overview
The 100Gaj AI Chatbot is a backend service that provides a conversational interface for real estate inquiries. It can answer general questions about the company (e.g., "What services do you offer?") and perform specific property searches (e.g., "Find apartments in Delhi").
The core technology is a Large Language Model (LLM) agent that can understand natural language and use tools to access a property database and a knowledge base.
Live API Base URL: https://one00gaj-chatbot.onrender.com

2. API Endpoints
There are two primary endpoints available.
A. Health Check Endpoint
This is a simple endpoint to verify that the API server is live and reachable.

Endpoint: /
Method: GET
Full URL: https://one00gaj-chatbot.onrender.com/
Description: A simple health check to confirm the server is running.
Success Response (200 OK):{
    "status": "ok",
    "message": "100Gaj API is running. Use /api/v1/chat to interact."
}


Usage: The frontend can call this endpoint on page load to show a "Connected" status indicator to the user.

B. Main Chat Endpoint
This is the core endpoint for all conversational interactions. It's a streaming API, which means it sends back the response in chunks as it's being generated by the AI, allowing for a "typing" effect on the frontend.

Endpoint: /api/v1/chat
Method: POST
Full URL: https://one00gaj-chatbot.onrender.com/api/v1/chat
Description: Sends a user's message and the conversation history to the AI agent and streams back the response.

Request Body (application/json)
The request must be a JSON object with the following structure:
{
  "message": "The user's new message as a string.",
  "history": [
    {
      "role": "user",
      "content": "The first message from the user."
    },
    {
      "role": "assistant",
      "content": "The AI's response to the first message."
    }
  ]
}


message (string, required): The latest message typed by the user.
history (array of objects, required): An array representing the conversation so far. It's crucial for providing context to the AI.
Each object in the array must have a role ("user" or "assistant") and content (the message text).
For the very first message of a new conversation, this can be an empty array [].



Response (text/event-stream)
The server responds with a Server-Sent Events (SSE) stream. The frontend needs to listen to this stream and process the data as it arrives. Each event is a line starting with data: .
There are two types of events:

text event: This contains a chunk of the AI's response.
data: {"type": "text", "data": "Here is the information you requested..."}


type: "text"
data: A string containing the full or partial response from the AI.


end event: This signals that the AI has finished generating its complete response for this turn.
data: {"type": "end"}


type: "end"
This event is the signal for the frontend to stop showing the "typing..." indicator.




3. Frontend Integration Workflow (JavaScript Example)
Here is a practical example of how to implement the chat functionality on the frontend using the fetch API with streaming.
// The live API endpoint
const API_URL = 'https://one00gaj-chatbot.onrender.com/api/v1/chat';

// This array must be maintained by the frontend to store the conversation
let chatHistory = [];

/**
 * Function to send a message to the API and handle the streamed response.
 * @param {string} userMessage - The message from the user.
 * @param {function} onChunkReceived - A callback function to handle each piece of the response.
 * @param {function} onStreamEnd - A callback function for when the stream is finished.
 * @param {function} onError - A callback function to handle errors.
 */
async function sendMessage(userMessage, onChunkReceived, onStreamEnd, onError) {
  try {
    const response = await fetch(API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Accept': 'text/event-stream' // Important: Tell the server we want a stream
      },
      body: JSON.stringify({
        message: userMessage,
        history: chatHistory // Send the current conversation history
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status: ${response.status}`);
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let fullAiResponse = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        onStreamEnd(fullAiResponse); // The stream is officially over
        break;
      }

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n\n').filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const dataStr = line.substring(6);
          
          try {
            const data = JSON.parse(dataStr);

            if (data.type === 'text') {
              fullAiResponse = data.data; // The AI sends the full text in each chunk
              onChunkReceived(fullAiResponse); // Update the UI with the latest text
            } else if (data.type === 'end') {
              // The 'end' event is a final confirmation.
              // The main `done` check above is the primary way to detect the end.
            }
          } catch (error) {
            console.error('Failed to parse stream data:', dataStr, error);
          }
        }
      }
    }

    // IMPORTANT: After a successful exchange, update the history for the next turn
    chatHistory.push({ role: 'user', content: userMessage });
    if (fullAiResponse) {
      chatHistory.push({ role: 'assistant', content: fullAiResponse });
    }

  } catch (error) {
    console.error('Fetch error:', error);
    onError(error.message);
  }
}

// Example usage in your chat UI
const form = document.getElementById('chat-form');
form.addEventListener('submit', (e) => {
  e.preventDefault();
  const userInput = document.getElementById('user-input');
  const message = userInput.value;
  userInput.value = '';

  // 1. Add user message to UI
  addUserMessageToChat(message);
  
  // 2. Create a placeholder for the AI response
  const aiMessageElement = addAiPlaceholder();
  showTypingIndicator(true);

  // 3. Call the API
  sendMessage(
    message,
    (chunk) => {
      // This is called every time a piece of data arrives
      aiMessageElement.textContent = chunk;
    },
    (finalResponse) => {
      // This is called when the entire message is finished
      showTypingIndicator(false);
    },
    (errorMessage) => {
      // This is called on network or server error
      aiMessageElement.textContent = `Error: ${errorMessage}`;
      showTypingIndicator(false);
    }
  );
});

4. Important Considerations for the Frontend

State Management: The frontend is responsible for maintaining the chatHistory array. For a new chat session, this array should be cleared.
Streaming UI: The best user experience is to display the AI's message as it streams in, creating a "live typing" effect. The onChunkReceived callback is perfect for this.
Error Handling: Gracefully handle network errors or cases where the server might return a non-200 status code. Display a user-friendly error message.
"Cold Start" on Free Tier: The first API call after a period of inactivity (e.g., 15 minutes) may take a long time to respond (up to 3 minutes) as the server "wakes up" and loads the AI model. The UI should show a persistent loading indicator during this time to inform the user. Subsequent requests will be much faster.
