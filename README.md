# 100Gaj AI Chatbot API Integration Guide 

## 1. Project Overview

The 100Gaj AI Chatbot is a backend service that provides a conversational interface for real estate inquiries. It can answer general questions about the company and perform specific property searches using natural language.

The core technology is a Large Language Model (LLM) agent that uses tools to access a property database and a knowledge base.

**Live API Base URL:** `https://100gaj-chatbot-production.up.railway.app`

---

## 2. API Endpoints

There are two primary endpoints available.

### A. Health Check Endpoint

This is a simple endpoint to verify that the API server is live and reachable.

*   **Endpoint:** `/`
*   **Method:** `GET`
*   **Full URL:** `https://100gaj-chatbot-production.up.railway.app/`
*   **Description:** A simple health check to confirm the server is running.
*   **Success Response (200 OK):**
    ```json
    {
        "status": "ok",
        "message": "100Gaj API is running. Use /api/v1/chat to interact."
    }
    ```
*   **Usage:** The frontend can call this endpoint on page load to show a "Connected" status indicator.

### B. Main Chat Endpoint

This is the core endpoint for all conversational interactions. It's a streaming API, which means it sends back the response in chunks as it's being generated by the AI, allowing for a "typing" effect on the frontend.

*   **Endpoint:** `/api/v1/chat`
*   **Method:** `POST`
*   **Full URL:** `https://100gaj-chatbot-production.up.railway.app/api/v1/chat`
*   **Description:** Sends a user's message and the conversation history to the AI agent and streams back the response.

#### Request Body (`application/json`)

The request must be a JSON object with the following structure:

```json
{
  "message": "The user's new message as a string.",
  "history": [
    {
      "role": "user",
      "content": "The first message from the user."
    },
    {
      "role": "assistant",
      "content": "The AI's response to the first message."
    }
  ]
}
```

*   **`message` (string, required):** The latest message typed by the user.
*   **`history` (array of objects, required):** An array representing the conversation so far. This is crucial for providing context to the AI.
    *   Each object must have a `role` (`"user"` or `"assistant"`) and `content` (the message text).
    *   For the very first message of a new conversation, this can be an empty array `[]`.

#### Response (`text/event-stream`)

The server responds with a Server-Sent Events (SSE) stream. Each event is a line starting with `data: `. The data within each event is a JSON string that must be parsed.

There are two types of events:

1.  **`text` event:** This contains a chunk of the AI's response.
    ```
    data: {"type": "text", "data": "Here is the information you requested..."}
    ```
    *   `type`: `"text"`
    *   `data`: A string containing the full or partial response from the AI.

2.  **`end` event:** This signals that the AI has finished generating its complete response for this turn.
    ```
    data: {"type": "end"}
    ```
    *   `type`: `"end"`
    *   This is the signal for the frontend to stop showing a "typing..." indicator.

---

## 3. Frontend Integration Workflow (JavaScript Example)

Here is a practical example of how to implement the chat functionality using the `fetch` API with streaming.

```javascript
// The live API endpoint on Railway
const API_URL = 'https://100gaj-chatbot-production.up.railway.app/api/v1/chat';

// This array must be maintained by the frontend to store the conversation
let chatHistory = [];

/**
 * Function to send a message to the API and handle the streamed response.
 * @param {string} userMessage - The message from the user.
 * @param {function} onChunkReceived - A callback function to handle each piece of the response.
 * @param {function} onStreamEnd - A callback function for when the stream is finished.
 * @param {function} onError - A callback function to handle errors.
 */
async function sendMessage(userMessage, onChunkReceived, onStreamEnd, onError) {
  try {
    const response = await fetch(API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Accept': 'text/event-stream' // Important: Tell the server we want a stream
      },
      body: JSON.stringify({
        message: userMessage,
        history: chatHistory // Send the current conversation history
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! Status: ${response.status}`);
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let fullAiResponse = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        onStreamEnd(fullAiResponse); // The stream is officially over
        break;
      }

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n\n').filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const dataStr = line.substring(6);
          try {
            const data = JSON.parse(dataStr);
            if (data.type === 'text') {
              fullAiResponse = data.data; // The AI sends the full text in each chunk
              onChunkReceived(fullAiResponse); // Update the UI with the latest text
            }
          } catch (error) {
            console.error('Failed to parse stream data:', dataStr, error);
          }
        }
      }
    }

    // IMPORTANT: After a successful exchange, update the history for the next turn
    chatHistory.push({ role: 'user', content: userMessage });
    if (fullAiResponse) {
      chatHistory.push({ role: 'assistant', content: fullAiResponse });
    }

  } catch (error) {
    console.error('Fetch error:', error);
    onError(error.message);
  }
}

// Example usage in your chat UI
/*
const form = document.getElementById('chat-form');
form.addEventListener('submit', (e) => {
  e.preventDefault();
  const userInput = document.getElementById('user-input');
  const message = userInput.value;
  userInput.value = '';

  addUserMessageToChat(message);
  const aiMessageElement = addAiPlaceholder();
  showTypingIndicator(true);

  sendMessage(
    message,
    (chunk) => {
      // Called every time a piece of data arrives
      aiMessageElement.textContent = chunk;
    },
    (finalResponse) => {
      // Called when the entire message is finished
      showTypingIndicator(false);
    },
    (errorMessage) => {
      // Called on network or server error
      aiMessageElement.textContent = `Error: ${errorMessage}`;
      showTypingIndicator(false);
    }
  );
});
*/
```

---

## 4. Important Considerations for the Frontend

*   **State Management:** The frontend is responsible for maintaining the `chatHistory` array. For a new chat session, this array should be cleared.

*   **Streaming UI:** The best user experience is to display the AI's message as it streams in, creating a "live typing" effect. The `onChunkReceived` callback is perfect for this.

*   **Error Handling:** Gracefully handle network errors or cases where the server might return a non-200 status code. Display a user-friendly error message.

*   **"Cold Start" on Free Tier:** The first API call after a period of inactivity may be slow as the server "wakes up" and loads the AI model. This can take up to 3 minutes. The UI should show a persistent loading indicator during this time to inform the user. Subsequent requests while the service is active will be much faster.
